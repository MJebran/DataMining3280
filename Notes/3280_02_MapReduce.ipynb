{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced18847-16cd-492a-a57d-8fb26c042881",
   "metadata": {},
   "source": [
    "# 02 MapReduce\n",
    "__Math 3280 - Data Mining__ : Snow College : Dr. Michael E. Olson\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937ebc0-9cd5-4895-bda7-0335ace5c02d",
   "metadata": {},
   "source": [
    "In this chapter, we look at the computer requirements when dealing with big data.\n",
    "\n",
    "When dealing with large computations, such as large-scale models, one computer will not be enough.\n",
    "> As an undergraduate, I created a model of air pollution in North Salt Lake. It was a 24-hour model covering a 100-km^2 area. It took well over 1 hour on my computer to get the results. Imagine how much more time it would have taken as a 3-Dimensional model covering the entire planet... By the time my computer finished a forecast model, the event being forecasted would have happened weeks ago.\n",
    "\n",
    "To handle large computers, we utilize __parallel processing__, where several processors are linked together and work on parts of the problem simultaneously. This helps the calculations to complete in far less time.\n",
    "* Each processor is called a __node__.\n",
    "* The collection of nodes is called a __supercomputer__.\n",
    "\n",
    "In data science, however, we are not only dealing with large computations, but with large amounts of data as well. \n",
    "* For example, large-scale Web services, such as Google or Amazon, are continually dealing with large amounts of data and customer interactions.\n",
    "\n",
    "To handle this, we use not only the processors on each node, but the storage space as well. \n",
    "* These systems are known as __computing clusters__.\n",
    "* The software to manage the data and queries is a __distributed file system__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce85953-c034-49a2-80be-7d36c4443ea2",
   "metadata": {},
   "source": [
    "## Cluster Computing and the Distributed File System\n",
    "Each node is installed on a __rack__. There are often 8-64 nodes on a rack. Each node on that rack is connected by a localized network - typically a gigabit ethernet.\n",
    "\n",
    "Several racks are then connected by another level of network or a switch. In order to get all the information from the racks to work with each other, they need more bandwidth than the rack itself has. We will learn about how these are used soon. First, let's look at the hardware challenges.\n",
    "\n",
    "All hardware eventually fails. With heavy usage, it will fail faster. \n",
    "* In large-scale services, one node can last about 3 years (a little more than 1000 days)\n",
    "* If I have a server of 1000 nodes, that means that on average, 1 node will fail every day\n",
    "* A server at Google may have a million nodes, which means there are about 1000 nodes that fail every day\n",
    "\n",
    "With so many failures, we have to ensure no disruption in data or in calculations if the failure happens while the program is running. To ensure this happens, there are two requirements:\n",
    "1. Files must be stored redundantly\n",
    "2. Computations must be divided into smaller tasks\n",
    "    * If one task fails, then only that one task needs to be restarted, not the entire program\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf078b57-509e-41ee-a413-b0ed3ea7f3f2",
   "metadata": {},
   "source": [
    "### The Distributed File System Organization\n",
    "A distributed file system (DFS) works by dividing the data file into separate pieces and copying them.\n",
    "1. Files are divided into __chunks__, typically 64 MB\n",
    "    * Size can be determined by the user\n",
    "2. Each chunk is saved on different nodes\n",
    "3. Each chunk is the replicated and saved on different nodes, perhaps 3 times\n",
    "    * Number of copies can be determined by the user\n",
    "    * The nodes holding the copies should be on different racks so copies aren't all lost if a rack fails\n",
    "4. A __master node__ (or name node) tracks the location of all chunks so retrieval is simplified\n",
    "    * The master node is also replicated\n",
    "\n",
    "A DFS is often used when,\n",
    "* individual files are large (terabytes), and\n",
    "* files are rarely updated\n",
    "\n",
    "There is no need for files to be distributed if they are small. And if any file is frequently updated, then the process becomes very complicated. So, this may be a good system for data on a global scale, but wouldn't work well for Amazon who has changes in inventory and prices daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90619b-77a0-47f8-a9c7-957873f3a454",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "__MapReduce__ is the style of computing that is used to implement the DFS methodology. There are many different implementations:\n",
    "* (GFS) Google File System - The original\n",
    "* (HDFS) Hadoop DFS - Open-source, distributed by the Apache Software Foundation\n",
    "* Spark\n",
    "* Colossus - An improved version of GFS\n",
    "\n",
    "The MapReduce process only involves two functions: *Map* and *Reduce*. The process is as follows. We'll follow the process with an example of counting the number of words.\n",
    "1. *Map tasks* are given one or more chunks from the DFS and matches it into key-value pairs\n",
    "    * Each map task looks for the words $w_1$, $w_2$, etc.\n",
    "    * The key-value pair would be ($w_1$, 1), ($w_2$, 1), etc.\n",
    "    * The result is a list of all key-value pairs ($w$, 1) for all documents\n",
    "2. A __master controller__ sorts these key-value pairs and assigns them to a *Reduce task* \n",
    "    * All word pairs are sorted as ($w_1$, 1), ($w_1$, 1), ($w_1$, 1), ... , ($w_1$, 1), ($w_2$, 1), ($w_2$, 1), ... ($w_2$, 1), ($w_3$, 1), ...\n",
    "    * All pairs with $w_1$ are given to one reduce task, $w_2$ to another, etc.\n",
    "      * Input to ReduceTask1: ($w_1$, [1,1,1,1])\n",
    "      * Input to ReduceTask2: ($w_2$, [1,1,1,1,1])\n",
    "      * Input to ReduceTask3: ($w_3$, [1,1]), ($w_4$, [1,1,1])\n",
    "      * ...\n",
    "3. *Reduce Tasks* work with one key at a time, combining the values associated with that key in some way\n",
    "    * Add all the values together\n",
    "      * Output from ReduceTask1: ($w_1$, 4)\n",
    "      * Output from ReduceTask2: ($w_2$, 5)\n",
    "      * Output from ReduceTask3: ($w_3$, 2), ($w_4$, 3)\n",
    "      \n",
    "What happens if a node fails in the middle?\n",
    "* Best case scenario: only a single map task or reduce task needs to be restarted\n",
    "* Worst cast scenario: the node at which the Master is executing fails, and the entire MapReduce job needs to be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa053e-45df-4dab-a00f-b3f8d4230bd4",
   "metadata": {},
   "source": [
    "## Algorithms using MapReduce\n",
    "* Matrix-Vector Multiplication\n",
    "\n",
    "\n",
    "### Matrix-Vector Multiplication with small vectors\n",
    "\n",
    "### Matrix-Vector Multiplication with large vectors\n",
    "\n",
    "### Relational Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad78dba-22fa-4c6e-9ff1-c18bc4ea12d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40768361-eff3-42ab-b8a0-a566a75dc74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e7141-403e-49f7-ae65-42e8c6552d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e462bdb-d322-43aa-9bd0-95fcb320b4be",
   "metadata": {},
   "source": [
    "## Homework\n",
    "1. Exercise 2.2.1 (a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
